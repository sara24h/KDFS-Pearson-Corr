import torch
import torch.nn as nn
import sys
sys.path.append('/kaggle/working')

from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal

# ===========================
# Ù…Ø³ÛŒØ± Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø¬Ø¯ÛŒØ¯
# ===========================
checkpoint_path = '/kaggle/input/kdfs-140k-pearson-19-shahrivar-data/results/run_resnet50_imagenet_prune1/student_model/resnet50_sparse_best.pt'

print("="*70)
print("ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§")
print("="*70)

# Ù„ÙˆØ¯ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª
checkpoint = torch.load(checkpoint_path, map_location='cpu')

# Ú†Ú© Ú©Ø±Ø¯Ù† Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
print(f"âœ… Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª: {list(checkpoint.keys())}")

# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† state_dict
if 'student' in checkpoint:
    sparse_state_dict = checkpoint['student']
    print("âœ… state_dict Ø¯Ø± Ú©Ù„ÛŒØ¯ 'student' Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
elif 'model_state_dict' in checkpoint:
    sparse_state_dict = checkpoint['model_state_dict']
    print("âœ… state_dict Ø¯Ø± Ú©Ù„ÛŒØ¯ 'model_state_dict' Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
elif 'state_dict' in checkpoint:
    sparse_state_dict = checkpoint['state_dict']
    print("âœ… state_dict Ø¯Ø± Ú©Ù„ÛŒØ¯ 'state_dict' Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
else:
    sparse_state_dict = checkpoint
    print("âœ… Ø®ÙˆØ¯ checkpoint Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† state_dict Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")

# ===========================
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§
# ===========================
def auto_extract_pruned_filters(state_dict):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø§Ø² ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ conv
    """
    pruned_filters = []
    
    layer_configs = [
        ('layer1', 3),  # 3 blocks
        ('layer2', 4),  # 4 blocks
        ('layer3', 6),  # 6 blocks
        ('layer4', 3),  # 3 blocks
    ]
    
    print("\nğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡:")
    print("-" * 70)
    
    for layer_name, num_blocks in layer_configs:
        print(f"\n{layer_name}:")
        for block_idx in range(num_blocks):
            block_filters = []
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                if key in state_dict:
                    num_filters = state_dict[key].shape[0]
                    block_filters.append(num_filters)
                    pruned_filters.append(num_filters)
            
            if block_filters:
                print(f"  Block {block_idx}: {block_filters[0]:3d}, {block_filters[1]:3d}, {block_filters[2]:3d}")
    
    print("-" * 70)
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§: {len(pruned_filters)}")
    
    return pruned_filters

pruned_filters = auto_extract_pruned_filters(sparse_state_dict)

# ===========================
# Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§
# ===========================
print("\n" + "="*70)
print("ğŸ­ Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§")
print("="*70)

def create_masks_from_pruned_filters(pruned_filters):
    """
    Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ pruned Ø´Ø¯Ù‡
    """
    # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¯Ø± ResNet50 Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    original_filters = [
        # layer1: 3 blocks
        64, 64, 256,  # block 0
        64, 64, 256,  # block 1
        64, 64, 256,  # block 2
        # layer2: 4 blocks
        128, 128, 512,  # block 0
        128, 128, 512,  # block 1
        128, 128, 512,  # block 2
        128, 128, 512,  # block 3
        # layer3: 6 blocks
        256, 256, 1024,  # block 0
        256, 256, 1024,  # block 1
        256, 256, 1024,  # block 2
        256, 256, 1024,  # block 3
        256, 256, 1024,  # block 4
        256, 256, 1024,  # block 5
        # layer4: 3 blocks
        512, 512, 2048,  # block 0
        512, 512, 2048,  # block 1
        512, 512, 2048,  # block 2
    ]
    
    masks = []
    total_original = 0
    total_pruned = 0
    
    for orig_filters, pruned_count in zip(original_filters, pruned_filters):
        mask = torch.zeros(orig_filters)
        mask[:pruned_count] = 1
        masks.append(mask)
        
        total_original += orig_filters
        total_pruned += pruned_count
    
    sparsity = (1 - total_pruned / total_original) * 100
    
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡: {len(masks)}")
    print(f"ğŸ“Š ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {total_original:,}")
    print(f"ğŸ“Š ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {total_pruned:,}")
    print(f"âœ‚ï¸  Sparsity: {sparsity:.2f}%")
    
    return masks, original_filters

masks, original_filters = create_masks_from_pruned_filters(pruned_filters)

# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
print("\nğŸ” Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ (5 ØªØ§ÛŒ Ø§ÙˆÙ„):")
for i in range(min(5, len(masks))):
    remaining = int(masks[i].sum())
    total = len(masks[i])
    percentage = (remaining / total) * 100
    print(f"  Mask {i}: {remaining:3d}/{total:4d} ({percentage:5.1f}% Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡)")

# ===========================
# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned
# ===========================
print("\n" + "="*70)
print("ğŸ—ï¸  Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned")
print("="*70)

try:
    model_pruned = ResNet_50_pruned_hardfakevsreal(masks=masks)
    print("âœ… Ù…Ø¯Ù„ pruned Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯!")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    total_params = sum(p.numel() for p in model_pruned.parameters())
    trainable_params = sum(p.numel() for p in model_pruned.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    print(f"ğŸ“Š Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´: {trainable_params:,}")
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ ResNet50 Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 25.6M Ù¾Ø§Ø±Ø§Ù…ØªØ±)
    standard_params = 25_600_000
    reduction = (1 - total_params / standard_params) * 100
    print(f"ğŸ“‰ Ú©Ø§Ù‡Ø´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ ResNet50 Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯: {reduction:.2f}%")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ù…Ø¯Ù„: {e}")
    import traceback
    traceback.print_exc()
    raise

# ===========================
# Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned
# ===========================
print("\n" + "="*70)
print("ğŸ“¥ Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned")
print("="*70)

def load_pruned_weights(model_pruned, sparse_state_dict, masks):
    """
    Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ sparse Ø¯Ø± Ù…Ø¯Ù„ pruned
    """
    pruned_state_dict = {}
    
    # conv1 Ùˆ bn1 (Ù‚Ø¨Ù„ Ø§Ø² layer1)
    for key in ['conv1.weight', 'bn1.weight', 'bn1.bias', 
                'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    # fc (Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±)
    for key in ['fc.weight', 'fc.bias']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    mask_idx = 0
    layer_configs = [
        ('layer1', 3),
        ('layer2', 4),
        ('layer3', 6),
        ('layer4', 3),
    ]
    
    for layer_name, num_blocks in layer_configs:
        for block_idx in range(num_blocks):
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                sparse_conv_key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                
                if sparse_conv_key in sparse_state_dict:
                    sparse_weight = sparse_state_dict[sparse_conv_key]
                    mask = masks[mask_idx]
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„
                    active_filters = (mask == 1).nonzero(as_tuple=True)[0]
                    pruned_weight = sparse_weight[active_filters]
                    
                    # Prune Ú©Ø±Ø¯Ù† input channels
                    if conv_idx > 1 and mask_idx > 0:
                        prev_mask = masks[mask_idx - 1]
                        active_in_channels = (prev_mask == 1).nonzero(as_tuple=True)[0]
                        pruned_weight = pruned_weight[:, active_in_channels]
                    
                    pruned_state_dict[sparse_conv_key] = pruned_weight
                    
                    # BatchNorm parameters
                    bn_prefix = f'{layer_name}.{block_idx}.bn{conv_idx}'
                    for bn_suffix in ['.weight', '.bias', '.running_mean', '.running_var', '.num_batches_tracked']:
                        bn_key = bn_prefix + bn_suffix
                        if bn_key in sparse_state_dict:
                            if 'num_batches_tracked' in bn_suffix:
                                pruned_state_dict[bn_key] = sparse_state_dict[bn_key]
                            else:
                                pruned_state_dict[bn_key] = sparse_state_dict[bn_key][active_filters]
                    
                    mask_idx += 1
            
            # downsample
            downsample_prefix = f'{layer_name}.{block_idx}.downsample'
            for ds_key in sparse_state_dict.keys():
                if ds_key.startswith(downsample_prefix):
                    pruned_state_dict[ds_key] = sparse_state_dict[ds_key]
    
    return pruned_state_dict

try:
    pruned_weights = load_pruned_weights(model_pruned, sparse_state_dict, masks)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ pruned Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯: {len(pruned_weights)} Ú©Ù„ÛŒØ¯")
    
    # Ù„ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„
    missing, unexpected = model_pruned.load_state_dict(pruned_weights, strict=False)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!")
    
    if missing:
        print(f"âš ï¸  Missing keys ({len(missing)}): {missing[:5]}...")
    if unexpected:
        print(f"âš ï¸  Unexpected keys ({len(unexpected)}): {unexpected[:5]}...")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§: {e}")
    import traceback
    traceback.print_exc()
    raise

# ===========================
# ØªØ³Øª Ù…Ø¯Ù„
# ===========================
print("\n" + "="*70)
print("ğŸ§ª ØªØ³Øª Ù…Ø¯Ù„")
print("="*70)

try:
    model_pruned.eval()
    with torch.no_grad():
        dummy_input = torch.randn(2, 3, 224, 224)
        output, features = model_pruned(dummy_input)
        
        print(f"âœ… ØªØ³Øª Ù…ÙˆÙÙ‚!")
        print(f"   ğŸ“Š Ø´Ú©Ù„ ÙˆØ±ÙˆØ¯ÛŒ: {dummy_input.shape}")
        print(f"   ğŸ“Š Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ: {output.shape}")
        print(f"   ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ feature maps: {len(features)}")
        print(f"   ğŸ“Š Ø´Ú©Ù„ feature maps: {[f.shape for f in features[:3]]}...")
        
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {e}")
    import traceback
    traceback.print_exc()
    raise

# ===========================
# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned
# ===========================
print("\n" + "="*70)
print("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned")
print("="*70)

try:
    # Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
    save_path = '/kaggle/working/resnet50_pruned_model_140k.pt'
    save_path_weights = '/kaggle/working/resnet50_pruned_weights_only_140k.pt'
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ù…Ù„ (Ø¨Ø§ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§)
    checkpoint_to_save = {
        'model_state_dict': model_pruned.state_dict(),
        'masks': masks,
        'pruned_filters': pruned_filters,
        'original_filters': original_filters,
        'total_params': total_params,
        'model_architecture': 'ResNet_50_pruned_hardfakevsreal',
        'source_checkpoint': checkpoint_path
    }
    
    torch.save(checkpoint_to_save, save_path)
    print(f"âœ… Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {save_path}")
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§
    torch.save(model_pruned.state_dict(), save_path_weights)
    print(f"âœ… ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {save_path_weights}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    import os
    file_size_mb = os.path.getsize(save_path) / (1024 * 1024)
    file_size_weights_mb = os.path.getsize(save_path_weights) / (1024 * 1024)
    
    print(f"\nğŸ“¦ Ø­Ø¬Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§:")
    print(f"   - Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„: {file_size_mb:.2f} MB")
    print(f"   - ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§: {file_size_weights_mb:.2f} MB")
    
    # Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡:")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§: {len(masks)}")
    print(f"   - Sparsity: {(1 - sum(pruned_filters) / sum(original_filters)) * 100:.2f}%")
    print(f"   - Ù…Ø¹Ù…Ø§Ø±ÛŒ: ResNet_50_pruned_hardfakevsreal")
    
    print("\nğŸ’¡ Ù†Ø­ÙˆÙ‡ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù†:")
    print("```python")
    print("# Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„")
    print(f"checkpoint = torch.load('{save_path}')")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=checkpoint['masks'])")
    print("model.load_state_dict(checkpoint['model_state_dict'])")
    print("\n# ÛŒØ§ Ù„ÙˆØ¯ Ø³Ø±ÛŒØ¹â€ŒØªØ±")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=masks)")
    print(f"model.load_state_dict(torch.load('{save_path_weights}'))")
    print("```")
    
    print("\nâœ… Ù‡Ù…Ù‡ Ú†ÛŒ ØªÙ…ÙˆÙ… Ø´Ø¯! Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª ğŸ‰")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
