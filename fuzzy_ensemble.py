import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix
from itertools import combinations
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
# Import ŸÖÿπŸÖÿßÿ±€å ŸÖÿØŸÑ pruned ÿ¥ŸÖÿß
from model.pruned_model.Resnet_final import ResNet_50_pruned_hardfakevsreal

class DeepfakeDataset(Dataset):
    """Dataset ÿ®ÿ±ÿß€å ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ™ÿµÿßŸà€åÿ±"""
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('RGB')
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
            
        return image, label

def get_transforms(model_name='default'):
    
    transforms_dict = {
        'model1': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5212, 0.4260, 0.3811], std=[0.2486, 0.2238, 0.2211])
        ]),
        'model2': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5207, 0.4258, 0.3806], std=[0.2490, 0.2239, 0.2212])
        ]),
        'model3': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.4868, 0.3972, 0.3624], std=[0.2296, 0.2066, 0.2009])
        ]),
        'model4': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.4668, 0.3816, 0.3414], std=[0.2410, 0.2161, 0.2081])
        ]),
        'model5': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.4923, 0.4042, 0.3624], std=[0.2446, 0.2198, 0.2141])
        ])
    }
    
    return transforms_dict.get(model_name, transforms_dict['default'])

# ==================== ÿ®ÿÆÿ¥ 2: ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Predictions ÿßÿ≤ ŸÖÿØŸÑ‚ÄåŸáÿß ====================
class ModelPredictor:
    
    def __init__(self, model_configs, device='cuda'):
        
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.models = []
        self.model_names = []
        
        print(f"üîß Device: {self.device}")
        print(f"üì¶ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å {len(model_configs)} ŸÖÿØŸÑ ResNet-50 Pruned...")
        
        for i, config in enumerate(model_configs):
            model_path = config['model_path']
            mask_path = config.get('mask_path', None)
            name = config.get('name', f'Model_{i+1}')
            
            print(f"\n [{i+1}/{len(model_configs)}] {name}")
            print(f" ŸÖÿØŸÑ: {os.path.basename(model_path)}")
            if mask_path:
                print(f" Mask: {os.path.basename(mask_path)}")
            else:
                print(f" Mask: None (ÿ®ÿØŸàŸÜ ŸÖÿßÿ≥⁄©)")
            
            model = self.load_pruned_model(model_path, mask_path)
            model.eval()
            
            self.models.append(model)
            self.model_names.append(name)
            
            print(f" ‚úì ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖŸàŸÅŸÇ")
        
        print(f"\n‚úÖ ŸáŸÖŸá ŸÖÿØŸÑ‚ÄåŸáÿß ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ¥ÿØŸÜÿØ!")
    
    def load_pruned_model(self, model_path, mask_path=None):
        
        if mask_path:
            mask_checkpoint = torch.load(mask_path, map_location='cpu')
        
            if isinstance(mask_checkpoint, dict):
                if 'mask' in mask_checkpoint:
                    masks = mask_checkpoint['mask']
                elif 'masks' in mask_checkpoint:
                    masks = mask_checkpoint['masks']
                else:
                    # ÿ¥ÿß€åÿØ ÿÆŸàÿØ checkpoint ŸáŸÖÿßŸÜ masks ÿ®ÿßÿ¥ÿØ
                    masks = mask_checkpoint
            else:
                masks = mask_checkpoint
        else:
            masks = None
        
        # ÿ≥ÿßÿÆÿ™ ŸÖÿØŸÑ ÿ®ÿß masks (ÿß⁄Øÿ± masks None ÿ®ÿßÿ¥ÿØÿå ŸÅÿ±ÿ∂ ÿ®ÿ± ÿß€åŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ŸÖÿØŸÑ ÿ®ÿØŸàŸÜ ŸÖÿßÿ≥⁄© ⁄©ÿßÿ± ŸÖ€å‚Äå⁄©ŸÜÿØ)
        model = ResNet_50_pruned_hardfakevsreal(masks=masks)
        
        # ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å Ÿàÿ≤ŸÜ‚ÄåŸáÿß
        model_checkpoint = torch.load(model_path, map_location=self.device)
        
        # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ state_dict
        if isinstance(model_checkpoint, dict):
            if 'model_state_dict' in model_checkpoint:
                state_dict = model_checkpoint['model_state_dict']
            elif 'state_dict' in model_checkpoint:
                state_dict = model_checkpoint['state_dict']
            elif 'model' in model_checkpoint:
                state_dict = model_checkpoint['model']
            else:
                state_dict = model_checkpoint
        else:
            state_dict = model_checkpoint
        
        # ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å Ÿàÿ≤ŸÜ‚ÄåŸáÿß
        model.load_state_dict(state_dict, strict=False)
        model = model.to(self.device)
        
        return model
    
    def get_predictions(self, image_paths, labels, batch_size=32, show_progress=True):
        
        n_samples = len(image_paths)
        n_models = len(self.models)
        all_predictions = np.zeros((n_samples, n_models))
        
        print(f"\nüîç ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å‚ÄåŸáÿß ÿßÿ≤ {n_models} ŸÖÿØŸÑ...")
        print(f" ÿ™ÿπÿØÿßÿØ ŸÜŸÖŸàŸÜŸá‚ÄåŸáÿß: {n_samples}")
        print(f" Batch size: {batch_size}")
        
        for model_idx, (model, model_name) in enumerate(zip(self.models, self.model_names)):
            print(f"\n üìä ŸÖÿØŸÑ {model_idx+1}/{n_models}: {model_name}")
            
            # ÿß€åÿ¨ÿßÿØ dataset ÿ®ÿß transform ŸÖÿÆÿµŸàÿµ ÿß€åŸÜ ŸÖÿØŸÑ
            transform = get_transforms(model_name)
            dataset = DeepfakeDataset(image_paths, labels, transform)
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=4,
                pin_memory=True if self.device.type == 'cuda' else False
            )
            
            # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ predictions
            predictions = []
            with torch.no_grad():
                iterator = tqdm(dataloader, desc=f" Processing") if show_progress else dataloader
                for images, _ in iterator:
                    images = images.to(self.device)
                    outputs, _ = model(images) # model returns (output, feature_list)
                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()
                    predictions.extend(probs)
            
            predictions = np.array(predictions)
            all_predictions[:, model_idx] = predictions
            
            # ŸÜŸÖÿß€åÿ¥ ÿ¢ŸÖÿßÿ±Ÿá‚ÄåŸáÿß
            print(f" üìà ŸÖ€åÿßŸÜ⁄Ø€åŸÜ: {np.mean(predictions):.4f}")
            print(f" üìâ ÿßŸÜÿ≠ÿ±ÿßŸÅ ŸÖÿπ€åÿßÿ±: {np.std(predictions):.4f}")
            print(f" üìè ÿØÿßŸÖŸÜŸá: [{np.min(predictions):.4f}, {np.max(predictions):.4f}]")
        
        print("\n‚úÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å‚ÄåŸáÿß ÿ™⁄©ŸÖ€åŸÑ ÿ¥ÿØ!")
        return all_predictions, np.array(labels)

# ==================== ÿ®ÿÆÿ¥ 3: Choquet Integral ====================
class SimplifiedChoquetIntegral(nn.Module):
    
    def __init__(self, n_models=5):
        super(SimplifiedChoquetIntegral, self).__init__()
        self.n_models = n_models
        
        # Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ŸÖÿ≥ÿ™ŸÇŸÑ ÿ®ÿ±ÿß€å Ÿáÿ± ŸÖÿØŸÑ
        self.individual_weights = nn.Parameter(torch.ones(n_models))
        
        # Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ÿ™ÿπÿßŸÖŸÑ ÿ≤Ÿàÿ¨€å (synergy)
        n_pairs = n_models * (n_models - 1) // 2
        self.interaction_weights = nn.Parameter(torch.zeros(n_pairs))
        
    def forward(self, predictions):
        """
        ÿ™ÿ±⁄©€åÿ® ŸÅÿßÿ≤€å Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å‚ÄåŸáÿß€å ŸÖÿØŸÑ‚ÄåŸáÿß
        
        Args:
            predictions: (batch_size, n_models) - ÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™ Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å ÿ¥ÿØŸá
        """
        batch_size = predictions.shape[0]
        
        # ŸÜÿ±ŸÖÿßŸÑ‚Äåÿ≥ÿßÿ≤€å Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ŸÖÿ≥ÿ™ŸÇŸÑ
        weights = torch.softmax(self.individual_weights, dim=0)
        
        # ÿ™ÿ±⁄©€åÿ® ÿÆÿ∑€å Ÿàÿ≤ŸÜ‚ÄåÿØÿßÿ±
        result = torch.sum(predictions * weights.unsqueeze(0), dim=1)
        
        # ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ÿ™ÿπÿßŸÖŸÑÿßÿ™ ÿ≤Ÿàÿ¨€å
        pair_idx = 0
        for i in range(self.n_models):
            for j in range(i+1, self.n_models):
                interaction = self.interaction_weights[pair_idx]
                result += interaction * predictions[:, i] * predictions[:, j]
                pair_idx += 1
        
        # ŸÖÿ≠ÿØŸàÿØ ⁄©ÿ±ÿØŸÜ ÿÆÿ±Ÿàÿ¨€å ÿ®Ÿá [0, 1]
        result = torch.sigmoid(result)
        
        return result

class EnsembleTrainer:
    """⁄©ŸÑÿßÿ≥ ÿ¢ŸÖŸàÿ≤ÿ¥ Ÿà ÿßÿ±ÿ≤€åÿßÿ®€å Ensemble ÿ®ÿß Choquet Integral"""
    
    def __init__(self, model_predictions, true_labels, model_names=None):
        """
        Args:
            model_predictions: ÿ¢ÿ±ÿß€åŸá numpy (n_samples, n_models)
            true_labels: ÿ¢ÿ±ÿß€åŸá numpy (n_samples,)
            model_names: ŸÑ€åÿ≥ÿ™ ŸÜÿßŸÖ ŸÖÿØŸÑ‚ÄåŸáÿß (ÿßÿÆÿ™€åÿßÿ±€å)
        """
        self.n_models = model_predictions.shape[1]
        self.model_names = model_names or [f'Model {i+1}' for i in range(self.n_models)]
        
        # ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá tensor
        self.X = torch.FloatTensor(model_predictions)
        self.y = torch.FloatTensor(true_labels)
        
        # ÿ™ŸÇÿ≥€åŸÖ ÿ®Ÿá train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            self.X.numpy(), self.y.numpy(),
            test_size=0.2, random_state=42, stratify=true_labels
        )
        
        self.X_train = torch.FloatTensor(X_train)
        self.y_train = torch.FloatTensor(y_train)
        self.X_val = torch.FloatTensor(X_val)
        self.y_val = torch.FloatTensor(y_val)
        
        # ÿ≥ÿßÿÆÿ™ ŸÖÿØŸÑ Choquet
        self.model = SimplifiedChoquetIntegral(self.n_models)
        self.history = {'train_loss': [], 'val_loss': [], 'val_auc': []}
        
    def train(self, epochs=100, lr=0.01, batch_size=256, patience=20):
        """ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿØŸÑ Choquet Integral"""
        criterion = nn.BCELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10, verbose=False
        )
        
        n_batches = len(self.X_train) // batch_size + 1
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("\n" + "="*70)
        print("üéì ÿ¥ÿ±Ÿàÿπ ÿ¢ŸÖŸàÿ≤ÿ¥ Choquet Integral")
        print("="*70)
        print(f"ŸÜŸÖŸàŸÜŸá‚ÄåŸáÿß€å ÿ¢ŸÖŸàÿ≤ÿ¥€å: {len(self.X_train):,}")
        print(f"ŸÜŸÖŸàŸÜŸá‚ÄåŸáÿß€å ÿßÿπÿ™ÿ®ÿßÿ±ÿ≥ŸÜÿ¨€å: {len(self.X_val):,}")
        print(f"ÿ™ÿπÿØÿßÿØ ŸÖÿØŸÑ‚ÄåŸáÿß: {self.n_models}")
        print(f"Epochs: {epochs}")
        print(f"Batch size: {batch_size}")
        print(f"Learning rate: {lr}")
        print(f"Patience: {patience}\n")
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0.0
            indices = torch.randperm(len(self.X_train))
            
            for i in range(n_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(self.X_train))
                if start_idx >= end_idx:
                    break
                
                batch_indices = indices[start_idx:end_idx]
                batch_X = self.X_train[batch_indices]
                batch_y = self.y_train[batch_indices]
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            # Validation
            self.model.eval()
            with torch.no_grad():
                val_outputs = self.model(self.X_val)
                val_loss = criterion(val_outputs, self.y_val).item()
                val_auc = roc_auc_score(self.y_val.numpy(), val_outputs.numpy())
            
            avg_train_loss = train_loss / n_batches
            self.history['train_loss'].append(avg_train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['val_auc'].append(val_auc)
            
            scheduler.step(val_loss)
            
            # ŸÜŸÖÿß€åÿ¥ Ÿæ€åÿ¥ÿ±ŸÅÿ™
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1:3d}/{epochs} | "
                      f"Train Loss: {avg_train_loss:.4f} | "
                      f"Val Loss: {val_loss:.4f} | "
                      f"Val AUC: {val_auc:.4f}")
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"\n‚ö†Ô∏è Early stopping ÿØÿ± epoch {epoch+1}")
                    break
        
        # ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ®Ÿáÿ™ÿ±€åŸÜ ŸÖÿØŸÑ
        self.model.load_state_dict(self.best_model_state)
        print("\n‚úÖ ÿ¢ŸÖŸàÿ≤ÿ¥ ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿ™⁄©ŸÖ€åŸÑ ÿ¥ÿØ!")
        print(f" ÿ®Ÿáÿ™ÿ±€åŸÜ Val Loss: {best_val_loss:.4f}")
        
    def evaluate(self, X_test, y_test):
        """ÿßÿ±ÿ≤€åÿßÿ®€å ÿ¨ÿßŸÖÿπ ŸÖÿØŸÑ ensemble"""
        self.model.eval()
        X_test_tensor = torch.FloatTensor(X_test)
        
        with torch.no_grad():
            ensemble_probs = self.model(X_test_tensor).numpy()
        
        ensemble_preds = (ensemble_probs >= 0.5).astype(int)
        
        # ŸÖÿ™ÿ±€å⁄©‚ÄåŸáÿß€å ensemble
        ensemble_acc = accuracy_score(y_test, ensemble_preds)
        ensemble_auc = roc_auc_score(y_test, ensemble_probs)
        ensemble_f1 = f1_score(y_test, ensemble_preds)
        
        # ÿßÿ±ÿ≤€åÿßÿ®€å ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ™⁄©€å
        individual_results = []
        for i in range(X_test.shape[1]):
            preds = (X_test[:, i] >= 0.5).astype(int)
            acc = accuracy_score(y_test, preds)
            auc = roc_auc_score(y_test, X_test[:, i])
            f1 = f1_score(y_test, preds)
            individual_results.append({
                'model': self.model_names[i],
                'accuracy': acc,
                'auc': auc,
                'f1': f1
            })
        
        best_single = max(individual_results, key=lambda x: x['auc'])
        
        # ŸÜŸÖÿß€åÿ¥ ŸÜÿ™ÿß€åÿ¨
        print("\n" + "="*70)
        print("üìä ŸÜÿ™ÿß€åÿ¨ ÿßÿ±ÿ≤€åÿßÿ®€å ŸÜŸáÿß€å€å")
        print("="*70)
        
        print("\nüîç ÿπŸÖŸÑ⁄©ÿ±ÿØ ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ™⁄©€å:")
        print("-" * 70)
        for result in individual_results:
            print(f"{result['model']:20s} | "
                  f"Acc: {result['accuracy']:.4f} | "
                  f"AUC: {result['auc']:.4f} | "
                  f"F1: {result['f1']:.4f}")
        
        print("\n" + "-" * 70)
        print(f"üèÜ ÿ®Ÿáÿ™ÿ±€åŸÜ ŸÖÿØŸÑ ÿ™⁄©€å: {best_single['model']}")
        print(f" AUC: {best_single['auc']:.4f}")
        
        print("\nüéØ ÿπŸÖŸÑ⁄©ÿ±ÿØ Ensemble (Choquet Integral):")
        print("-" * 70)
        print(f"Accuracy: {ensemble_acc:.4f}")
        print(f"AUC: {ensemble_auc:.4f}")
        print(f"F1-Score: {ensemble_f1:.4f}")
        
        print("\nüìà ÿ®Ÿáÿ®ŸàÿØ ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ÿ®Ÿáÿ™ÿ±€åŸÜ ŸÖÿØŸÑ ÿ™⁄©€å:")
        print("-" * 70)
        improvement_auc = ((ensemble_auc - best_single['auc']) / best_single['auc']) * 100
        improvement_acc = ((ensemble_acc - best_single['accuracy']) / best_single['accuracy']) * 100
        improvement_f1 = ((ensemble_f1 - best_single['f1']) / best_single['f1']) * 100
        
        print(f"AUC: {improvement_auc:+.2f}%")
        print(f"Accuracy: {improvement_acc:+.2f}%")
        print(f"F1-Score: {improvement_f1:+.2f}%")
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, ensemble_preds)
        print("\nüìã Confusion Matrix:")
        print(f" True Negative: {cm[0,0]:,}")
        print(f" False Positive: {cm[0,1]:,}")
        print(f" False Negative: {cm[1,0]:,}")
        print(f" True Positive: {cm[1,1]:,}")
        print("="*70)
        
        return {
            'ensemble': {'accuracy': ensemble_acc, 'auc': ensemble_auc, 'f1': ensemble_f1},
            'best_single': best_single,
            'individual': individual_results,
            'improvement': {
                'auc': improvement_auc,
                'accuracy': improvement_acc,
                'f1': improvement_f1
            }
        }
    
    def get_learned_weights(self):
        """ŸÜŸÖÿß€åÿ¥ Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å €åÿßÿØ⁄Ø€åÿ±€å‚Äåÿ¥ÿØŸá"""
        print("\n" + "="*70)
        print("‚öñÔ∏è Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å €åÿßÿØ⁄Ø€åÿ±€å‚Äåÿ¥ÿØŸá")
        print("="*70)
        
        weights = torch.softmax(self.model.individual_weights, dim=0).detach().numpy()
        print("\nüìä Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ŸÖÿ≥ÿ™ŸÇŸÑ ŸÖÿØŸÑ‚ÄåŸáÿß:")
        for i, (name, w) in enumerate(zip(self.model_names, weights)):
            bar = '‚ñà' * int(w * 50)
            print(f" {name:20s}: {w:.4f} ({w*100:5.1f}%) {bar}")
        
        print("\nüîó Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ÿ™ÿπÿßŸÖŸÑ ÿ≤Ÿàÿ¨€å:")
        interactions = self.model.interaction_weights.detach().numpy()
        idx = 0
        for i in range(self.n_models):
            for j in range(i+1, self.n_models):
                sign = "+" if interactions[idx] >= 0 else ""
                color = "üü¢" if interactions[idx] > 0.01 else "üî¥" if interactions[idx] < -0.01 else "‚ö™"
                print(f" {color} {self.model_names[i]} ‚Üî {self.model_names[j]}: {sign}{interactions[idx]:.4f}")
                idx += 1
        print("="*70)
    
    def plot_training_history(self, save_path='training_history.png'):
        """ÿ±ÿ≥ŸÖ ÿ™ÿßÿ±€åÿÆ⁄ÜŸá ÿ¢ŸÖŸàÿ≤ÿ¥"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        axes[0].plot(self.history['train_loss'], label='Train Loss', linewidth=2, color='#1f77b4')
        axes[0].plot(self.history['val_loss'], label='Validation Loss', linewidth=2, color='#ff7f0e')
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].set_title('Training History - Loss', fontsize=14, fontweight='bold')
        axes[0].legend(fontsize=11)
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(self.history['val_auc'], label='Validation AUC', color='#2ca02c', linewidth=2)
        axes[1].set_xlabel('Epoch', fontsize=12)
        axes[1].set_ylabel('AUC', fontsize=12)
        axes[1].set_title('Training History - AUC', fontsize=14, fontweight='bold')
        axes[1].legend(fontsize=11)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nüìä ŸÜŸÖŸàÿØÿßÿ± ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ: {save_path}")
        plt.show()
    
    def save_model(self, save_path='choquet_ensemble.pth'):
        """ÿ∞ÿÆ€åÿ±Ÿá ŸÖÿØŸÑ ÿ¢ŸÖŸàÿ≤ÿ¥‚ÄåÿØ€åÿØŸá"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_names': self.model_names,
            'n_models': self.n_models,
            'history': self.history
        }, save_path)
        print(f"üíæ ŸÖÿØŸÑ ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ: {save_path}")
    
    def load_model(self, load_path='choquet_ensemble.pth'):
        """ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ ÿ∞ÿÆ€åÿ±Ÿá‚Äåÿ¥ÿØŸá"""
        checkpoint = torch.load(load_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model_names = checkpoint['model_names']
        self.history = checkpoint.get('history', {'train_loss': [], 'val_loss': [], 'val_auc': []})
        print(f"üìÇ ŸÖÿØŸÑ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ¥ÿØ: {load_path}")

# ==================== ÿ®ÿÆÿ¥ 4: Pipeline ÿßÿµŸÑ€å ====================
def main_pipeline(model_configs, image_paths, labels, test_size=0.15):
    """
    Pipeline ⁄©ÿßŸÖŸÑ: ÿßÿ≤ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ‚ÄåŸáÿß ÿ™ÿß ÿßÿ±ÿ≤€åÿßÿ®€å ŸÜŸáÿß€å€å
    
    Args:
        model_configs: ŸÑ€åÿ≥ÿ™ ÿØ€å⁄©ÿ¥ŸÜÿ±€å‚ÄåŸáÿß€å ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ŸÖÿØŸÑ‚ÄåŸáÿß
        image_paths: ŸÑ€åÿ≥ÿ™ ŸÖÿ≥€åÿ±Ÿáÿß€å ÿ™ÿµÿßŸà€åÿ±
        labels: ÿ¢ÿ±ÿß€åŸá numpy ÿ®ÿ±⁄Üÿ≥ÿ®‚ÄåŸáÿß
        test_size: ŸÜÿ≥ÿ®ÿ™ ÿØÿßÿØŸá‚ÄåŸáÿß€å ÿ™ÿ≥ÿ™
    
    Returns:
        trainer: ÿ¥€åÿ° EnsembleTrainer
        results: ŸÜÿ™ÿß€åÿ¨ ÿßÿ±ÿ≤€åÿßÿ®€å
    """
    
    print("\n" + "="*70)
    print("üöÄ ÿ≥€åÿ≥ÿ™ŸÖ ÿ™ÿ¥ÿÆ€åÿµ Deepfake ÿ®ÿß Choquet Integral")
    print(" ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ŸÖÿØŸÑ‚ÄåŸáÿß€å ResNet-50 Pruned")
    print("="*70)
    
    # ⁄ØÿßŸÖ 1: ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ‚ÄåŸáÿß
    print("\nüì• ⁄ØÿßŸÖ 1: ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ‚ÄåŸáÿß€å ResNet-50 Pruned")
    predictor = ModelPredictor(model_configs, device='cuda')
    
    # ⁄ØÿßŸÖ 2: ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ predictions
    print("\nüîç ⁄ØÿßŸÖ 2: ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å‚ÄåŸáÿß€å ŸÖÿØŸÑ‚ÄåŸáÿß")
    all_predictions, all_labels = predictor.get_predictions(
        image_paths, labels, batch_size=32, show_progress=True
    )
    
    # ⁄ØÿßŸÖ 3: ÿ™ŸÇÿ≥€åŸÖ ÿØÿßÿØŸá
    print("\n‚úÇÔ∏è ⁄ØÿßŸÖ 3: ÿ™ŸÇÿ≥€åŸÖ ÿØÿßÿØŸá ÿ®Ÿá Train/Test")
    X_train, X_test, y_train, y_test = train_test_split(
        all_predictions, all_labels,
        test_size=test_size, random_state=42, stratify=all_labels
    )
    print(f" ÿ¢ŸÖŸàÿ≤ÿ¥: {len(X_train):,} ŸÜŸÖŸàŸÜŸá ({(1-test_size)*100:.0f}%)")
    print(f" ÿ™ÿ≥ÿ™: {len(X_test):,} ŸÜŸÖŸàŸÜŸá ({test_size*100:.0f}%)")
    
    # ⁄ØÿßŸÖ 4: ÿ¢ŸÖŸàÿ≤ÿ¥ Ensemble
    print("\nüéì ⁄ØÿßŸÖ 4: ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿØŸÑ Ensemble")
    trainer = EnsembleTrainer(X_train, y_train, predictor.model_names)
    trainer.train(epochs=100, lr=0.01, batch_size=256, patience=20)
    
    # ⁄ØÿßŸÖ 5: ÿßÿ±ÿ≤€åÿßÿ®€å
    print("\nüìä ⁄ØÿßŸÖ 5: ÿßÿ±ÿ≤€åÿßÿ®€å Ÿà ŸÜŸÖÿß€åÿ¥ ŸÜÿ™ÿß€åÿ¨")
    trainer.get_learned_weights()
    results = trainer.evaluate(X_test, y_test)
    trainer.plot_training_history()
    
    # ⁄ØÿßŸÖ 6: ÿ∞ÿÆ€åÿ±Ÿá ŸÖÿØŸÑ
    print("\nüíæ ⁄ØÿßŸÖ 6: ÿ∞ÿÆ€åÿ±Ÿá ŸÖÿØŸÑ ŸÜŸáÿß€å€å")
    trainer.save_model('choquet_ensemble_final.pth')
    
    print("\n" + "="*70)
    print("‚úÖ ŸÅÿ±ÿ¢€åŸÜÿØ ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ⁄©ÿßŸÖŸÑ ÿ¥ÿØ!")
    print("="*70)
    
    return trainer, results

# ==================== ÿ®ÿÆÿ¥ 5: ŸÜÿ≠ŸàŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ====================
if __name__ == "__main__":
    import glob  # ÿ®ÿ±ÿß€å ÿ¨ŸÖÿπ‚Äåÿ¢Ÿàÿ±€å ŸÖÿ≥€åÿ± ÿ™ÿµÿßŸà€åÿ±
    
    # ÿ™ÿπÿ±€åŸÅ ŸÖÿØŸÑ‚ÄåŸáÿß (ŸÖÿ≥€åÿ±Ÿáÿß ÿ±ÿß ÿ®ÿß ŸÖÿ≥€åÿ±Ÿáÿß€å ŸàÿßŸÇÿπ€å ÿÆŸàÿØ ÿ¨ÿß€å⁄Øÿ≤€åŸÜ ⁄©ŸÜ€åÿØ)
    model_configs = [
        {
            'name': 'model1',
            'model_path': '/kaggle/input/10k-pearson-pruned/pytorch/default/1/10k_pearson_pruned.pt',
        },
        {
            'name': 'model2',
            'model_path': '/kaggle/input/140k-pearson-pruned/pytorch/default/1/140k_pearson_pruned.pt',
        },
        {
            'name': 'model3',
            'model_path': '/kaggle/input/200k-pearson-pruned/pytorch/default/1/200k_kdfs_pruned.pt',
        },
        {
            'name': 'model4',
            'model_path': '/kaggle/input/190k-pearson-pruned/pytorch/default/1/190k_pearson_pruned.pt',
        },
        {
            'name': 'model5',
            'model_path': '/kaggle/input/330k-base-pruned/pytorch/default/1/330k_base_pruned.pt',
        }
    ]
    
    # ÿ™ÿπÿ±€åŸÅ ÿØ€åÿ™ÿßÿ≥ÿ™ (ŸÖÿ≥€åÿ±Ÿáÿß ÿ±ÿß ÿ®ÿß ŸÖÿ≥€åÿ±Ÿáÿß€å ŸàÿßŸÇÿπ€å ÿÆŸàÿØ ÿ¨ÿß€å⁄Øÿ≤€åŸÜ ⁄©ŸÜ€åÿØ)
    real_images = glob.glob('//kaggle/input/20k-wild-deepfake-dataset/wild-dataset_20k/valid/real/*.png')  # €åÿß Ÿáÿ± ŸÅÿ±ŸÖÿ™
    fake_images = glob.glob('//kaggle/input/20k-wild-deepfake-dataset/wild-dataset_20k/valid/fake/*.png')
    
    image_paths = real_images + fake_images
    labels = np.array([0] * len(real_images) + [1] * len(fake_images))  # €∞ ÿ®ÿ±ÿß€å ŸàÿßŸÇÿπ€åÿå €± ÿ®ÿ±ÿß€å ŸÅ€å⁄©
    
    # ÿßÿ¨ÿ±ÿß€å pipeline
    trainer, results = main_pipeline(
        model_configs=model_configs,
        image_paths=image_paths,
        labels=labels,
        test_size=0.15  # ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿ™ÿ∫€å€åÿ± ÿØŸá€åÿØ
    )
    
    # ÿß⁄Øÿ± ŸÖ€å‚ÄåÿÆŸàÿßŸá€åÿØ ŸÖÿØŸÑ ÿ∞ÿÆ€åÿ±Ÿá‚Äåÿ¥ÿØŸá ÿ±ÿß ÿ®ÿπÿØÿßŸã ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ⁄©ŸÜ€åÿØ:
    # trainer.load_model('choquet_ensemble_final.pth')
