import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import os
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, f1_score

# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù„Ø§Ø³ Ù…Ø¯Ù„ Ø´Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ù‚Ø§Ø¨Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø§Ø³Øª
from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal

def main():
    # --- 1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ---
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("======================================================================")
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Fine-tuning Ù…Ø¯Ù„ Pruned ResNet50")
    print("======================================================================")

    # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ (Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ)
    model_save_path = '/kaggle/working/resnet50_pruned_reloaded_saved.pt'

    # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
    data_dir = '/kaggle/input/wild-deepfake'
    train_dir = os.path.join(data_dir, 'train')
    val_dir = os.path.join(data_dir, 'valid')
    test_dir = os.path.join(data_dir, 'test')

    # --- 2. ØªØ¹Ø±ÛŒÙ ØªØ¨Ø¯ÛŒÙ„Ø§Øª (Transforms) ---
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'test': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    # --- 3. Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ---
    print("ğŸ“Š Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ DataLoaders...")
    try:
        image_datasets = {
            'train': datasets.ImageFolder(root=train_dir, transform=data_transforms['train']),
            'val': datasets.ImageFolder(root=val_dir, transform=data_transforms['val']),
            'test': datasets.ImageFolder(root=test_dir, transform=data_transforms['test']),
        }
        dataloaders = {
            'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=2, pin_memory=True),
            'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=2, pin_memory=True),
            'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=2, pin_memory=True),
        }
        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}
        class_names = image_datasets['train'].classes
        print(f"ğŸ“Š Dataset loaded: {dataset_sizes['train']} images ({dataset_sizes['train']//2} real, {dataset_sizes['train']//2} fake)")
        print(f"ğŸ“Š Dataset loaded: {dataset_sizes['val']} images ({dataset_sizes['val']//2} real, {dataset_sizes['val']//2} fake)")
        print(f"ğŸ“Š Dataset loaded: {dataset_sizes['test']} images ({dataset_sizes['test']//2} real, {dataset_sizes['test']//2} fake)")
    except FileNotFoundError as e:
        print(f"âŒ Error loading dataset: {e}")
        print(f"âŒ Check if paths exist: {train_dir}, {val_dir}, {test_dir}")
        return

    # --- 4. Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ù‡Ø±Ø³â€ŒØ´Ø¯Ù‡ ---
    print("ğŸ“¦ Ù„ÙˆØ¯ Ù…Ø¯Ù„ Pruned...")
    try:
        checkpoint = torch.load(model_save_path, map_location=device)
        masks = checkpoint['masks']

        model = ResNet_50_pruned_hardfakevsreal(masks=masks)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(device)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print("âœ… Ù…Ø¯Ù„ Ù„ÙˆØ¯ Ø´Ø¯")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´: {trainable_params:,}")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return

    # --- 5. ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² ---
    criterion = nn.CrossEntropyLoss()
    # Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©Ù… Ø¨Ø±Ø§ÛŒ Fine-tuning
    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4) # weight_decay Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…Ú© Ú©Ù†Ø¯
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    # --- 6. Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ (Fine-tuning) ---
    num_epochs = 20 # ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
    best_val_acc = 0.0
    best_model_wts = model.state_dict().copy()

    print("======================================================================")
    print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´")
    print("======================================================================")

    for epoch in range(num_epochs):
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nğŸ“ Epoch {epoch+1}/{num_epochs}")
        print(f"   Learning Rate: {current_lr:.6f}")
        print("-" * 70)

        # Ø¢Ù…ÙˆØ²Ø´
        train_loss, train_acc = train_epoch(model, dataloaders['train'], criterion, optimizer, device, scheduler, epoch, num_epochs)
        print(f"   Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}")

        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        val_loss, val_acc, val_f1, _, _ = val_epoch(model, dataloaders['val'], criterion, device, epoch, num_epochs)
        print(f"   Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}")

        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_wts = model.state_dict().copy()
            print("   -> Best model updated!")

    print(f"\nğŸ‰ Best val Acc: {best_val_acc:.4f}")

    # --- 7. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Fine-tune Ø´Ø¯Ù‡ ---
    model.load_state_dict(best_model_wts)
    fine_tuned_save_path = '/kaggle/working/resnet50_pruned_finetuned_wilddeepfake.pt'
    torch.save({
        'model_state_dict': model.state_dict(),
        'masks': checkpoint.get('masks'), # Ø§Ø² Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        'optimizer_state_dict': optimizer.state_dict(),
        'best_val_acc': best_val_acc,
        'epoch': epoch+1,
        'model_architecture': 'ResNet_50_pruned_hardfakevsreal_finetuned'
    }, fine_tuned_save_path)

    print(f"âœ… Fine-tuned model saved to {fine_tuned_save_path}")

    # --- 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª ---
    print("\n======================================================================")
    print("ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª")
    print("======================================================================")
    model.load_state_dict(best_model_wts) # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
    test_loss, test_acc, test_f1, test_labels, test_preds = val_epoch(model, dataloaders['test'], criterion, device, 'Final_Test', 0)

    print("\nFinal Evaluation on Test Set:")
    print(f"Accuracy: {test_acc:.4f}")
    print(f"F1-Score: {test_f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(test_labels, test_preds, target_names=class_names))
    print("\nğŸ‰ Ø§ØªÙ…Ø§Ù… ÙØ±Ø¢ÛŒÙ†Ø¯ Fine-tuning Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ.")


def train_epoch(model, dataloader, criterion, optimizer, device, scheduler, epoch, num_epochs):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    progress_bar = tqdm(dataloader, desc=f'Train Epoch {epoch+1}/{num_epochs}', leave=False)

    for inputs, labels in progress_bar:
        inputs = inputs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        optimizer.zero_grad()

        # Forward - Ø¨Ø¯ÙˆÙ† autocast
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        # Backward
        loss.backward()

        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data).item()

        progress_bar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{running_corrects / ((progress_bar.n + 1) * progress_bar.batch_size):.4f}'
        })

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = running_corrects / len(dataloader.dataset)
    return epoch_loss, epoch_acc

def val_epoch(model, dataloader, criterion, device, epoch, num_epochs):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(dataloader, desc=f'Val/Test Epoch {epoch+1}/{num_epochs}' if isinstance(epoch, int) else 'Testing', leave=False)
    with torch.no_grad():
        for inputs, labels in progress_bar:
            inputs = inputs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # Forward - Ø¨Ø¯ÙˆÙ† autocast
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data).item()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{running_corrects / ((progress_bar.n + 1) * progress_bar.batch_size):.4f}'
            })

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = running_corrects / len(dataloader.dataset)
    val_f1 = f1_score(all_labels, all_preds, average='weighted')
    return epoch_loss, epoch_acc, val_f1, all_labels, all_preds


if __name__ == "__main__":
    main()
