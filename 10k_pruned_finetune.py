import json
import os
import random
import time
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
from torch.amp import autocast, GradScaler
import argparse

from model.pruned_model.Resnet_final import ResNet_50_pruned_hardfakevsreal

# ============================================================
# 1. ØªØ¹Ø±ÛŒÙ Dataset Ø³ÙØ§Ø±Ø´ÛŒ
# ============================================================
class WildDeepfakeDataset(Dataset):
    def __init__(self, real_path, fake_path, transform=None):
        self.transform = transform
        self.images = []
        self.labels = []

        # Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Real (label = 0)
        if os.path.exists(real_path):
            real_files = [f for f in os.listdir(real_path) if f.endswith(('.jpg', '.jpeg', '.png'))]
            for fname in real_files:
                self.images.append(os.path.join(real_path, fname))
                self.labels.append(0)

        # Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Fake (label = 1)
        if os.path.exists(fake_path):
            fake_files = [f for f in os.listdir(fake_path) if f.endswith(('.jpg', '.jpeg', '.png'))]
            for fname in fake_files:
                self.images.append(os.path.join(fake_path, fname))
                self.labels.append(1)

        print(f"ğŸ“Š Dataset loaded: {len(self.images)} images ({sum(1 for l in self.labels if l==0)} real, {sum(1 for l in self.labels if l==1)} fake)")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]

        try:
            img = Image.open(img_path).convert('RGB')
            if self.transform:
                img = self.transform(img)
            # Ø¨Ø±Ø§ÛŒ BCEWithLogitsLoss Ø¨Ø§ num_classes=1ØŒ Ù„ÛŒØ¨Ù„ Ø¨Ø§ÛŒØ¯ float Ø¨Ø§Ø´Ø¯
            return img, torch.tensor(label, dtype=torch.float32)
        except Exception as e:
            print(f"âŒ Error loading {img_path}: {e}")
            return torch.zeros(3, 224, 224), torch.tensor(label, dtype=torch.float32)

# ============================================================
# 2. ØªØ¹Ø±ÛŒÙ Transforms
# ============================================================
train_transform = transforms.Compose([
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4414, 0.3448, 0.3159], std=[0.1854, 0.1623, 0.1562])
])

val_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4414, 0.3448, 0.3159], std=[0.1854, 0.1623, 0.1562])
])

# ============================================================
# 3. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ DataLoaders
# ============================================================
def create_dataloaders(batch_size=64, num_workers=4):
    train_dataset = WildDeepfakeDataset(
        real_path="/kaggle/input/wild-deepfake/train/real",
        fake_path="/kaggle/input/wild-deepfake/train/fake",
        transform=train_transform
    )

    val_dataset = WildDeepfakeDataset(
        real_path="/kaggle/input/wild-deepfake/valid/real",
        fake_path="/kaggle/input/wild-deepfake/valid/fake",
        transform=val_transform
    )

    test_dataset = WildDeepfakeDataset(
        real_path="/kaggle/input/wild-deepfake/test/real",
        fake_path="/kaggle/input/wild-deepfake/test/fake",
        transform=val_transform
    )

    # Ø§ÛŒØ¬Ø§Ø¯ DistributedSampler Ø¨Ø±Ø§ÛŒ DDP
    train_sampler = DistributedSampler(train_dataset)
    val_sampler = DistributedSampler(val_dataset, shuffle=False)
    test_sampler = DistributedSampler(test_dataset, shuffle=False)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,
                            num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler,
                          num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,
                           num_workers=num_workers, pin_memory=True)

    return train_loader, val_loader, test_loader, train_sampler, val_sampler, test_sampler

# ============================================================
# 4. ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ (Ø­Ø°Ù ACCUMULATION_STEPS)
# ============================================================
def train_epoch(model, loader, criterion, optimizer, device, scaler, writer, epoch, rank=0):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    # ÙÙ‚Ø· ÛŒÚ© Ù¾ÛŒØ´Ø±ÙØª Ù†Ø´Ø§Ù†Ú¯Ø± Ø¯Ø± ÛŒÚ© rank Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
    pbar = tqdm(loader, desc="Training", disable=rank != 0)

    for batch_idx, (inputs, labels) in enumerate(pbar):
        inputs, labels = inputs.to(device), labels.to(device)
        labels = labels.unsqueeze(1)

        with autocast(device_type='cuda', dtype=torch.float16):
            outputs, _ = model(inputs)
            loss = criterion(outputs, labels)

        # Ù…Ù‚ÛŒØ§Ø³â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¨Ú©â€ŒÙ¾Ø±Ø§Ù¾ Ø¨Ø¯ÙˆÙ† ØªØ¬Ù…Ø¹
        scaler.scale(loss).backward()

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ùˆ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad() # ØµÙØ± Ú©Ø±Ø¯Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ

        running_loss += loss.item()
        with torch.no_grad():
            preds = (torch.sigmoid(outputs) > 0.5).float()
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        if rank == 0:
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*correct/total:.2f}%'
            })

    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ Ø¨ÛŒÙ† ØªÙ…Ø§Ù… rank Ù‡Ø§
    avg_loss = torch.tensor(running_loss / len(loader)).to(device)
    avg_acc = torch.tensor(100. * correct / total).to(device)

    dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(avg_acc, op=dist.ReduceOp.SUM)

    avg_loss = avg_loss.item() / dist.get_world_size()
    avg_acc = avg_acc.item() / dist.get_world_size()

    # ÙÙ‚Ø· rank 0 writer Ø±Ø§ Ø¯Ø§Ø±Ø¯
    if rank == 0 and writer is not None:
        writer.add_scalar("train/loss", avg_loss, epoch)
        writer.add_scalar("train/acc", avg_acc, epoch)

    return avg_loss, avg_acc

@torch.no_grad()
def validate(model, loader, criterion, device, writer, epoch, rank=0):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in tqdm(loader, desc="Validation", disable=rank != 0):
        inputs, labels = inputs.to(device), labels.to(device)
        labels = labels.unsqueeze(1)

        with autocast(device_type='cuda', dtype=torch.float16):
            outputs, _ = model(inputs)
            loss = criterion(outputs, labels)

        running_loss += loss.item()
        preds = (torch.sigmoid(outputs) > 0.5).float()
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    avg_loss = torch.tensor(running_loss / len(loader)).to(device)
    avg_acc = torch.tensor(100. * correct / total).to(device)

    dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(avg_acc, op=dist.ReduceOp.SUM)

    avg_loss = avg_loss.item() / dist.get_world_size()
    avg_acc = avg_acc.item() / dist.get_world_size()

    # ÙÙ‚Ø· rank 0 writer Ø±Ø§ Ø¯Ø§Ø±Ø¯
    if rank == 0 and writer is not None:
        writer.add_scalar("val/loss", avg_loss, epoch)
        writer.add_scalar("val/acc", avg_acc, epoch)

    return avg_loss, avg_acc

# ============================================================
# 5. ØªØ§Ø¨Ø¹ setup DDP Ùˆ seed
# ============================================================
def setup_ddp(seed):
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')

    # Ù‚Ø§Ø¨Ù„ÛŒØª ØªÚ©Ø±Ø§Ø±
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
    torch.use_deterministic_algorithms(True)
    seed = seed + dist.get_rank()
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # Ø¨Ø±Ø§ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÚ©Ø±Ø§Ø±
    torch.backends.cudnn.enabled = True

    return local_rank

def cleanup_ddp():
    dist.destroy_process_group()

# ============================================================
# 6. Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Fine-tuning (Ø­Ø°Ù ACCUMULATION_STEPS)
# ============================================================
def main():
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† seed
    SEED = 42
    local_rank = setup_ddp(SEED)
    world_size = dist.get_world_size()
    global_rank = dist.get_rank()

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    DEVICE = torch.device(f"cuda:{local_rank}")
    BATCH_SIZE_PER_GPU = 256  # Ø§Ú¯Ø± OOM Ú¯Ø±ÙØªÛŒØ¯ØŒ Ø§ÛŒÙ† Ø±Ø§ Ú©Ù… Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹ 128 ÛŒØ§ 64)
    BATCH_SIZE = BATCH_SIZE_PER_GPU * world_size
    NUM_EPOCHS = 10
    LEARNING_RATE = 0.0001
    WEIGHT_DECAY = 1e-4
    # ACCUMULATION_STEPS = 2  # Ø­Ø°Ù Ø´Ø¯

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª TensorBoard
    result_dir = f'/kaggle/working/runs_ddp_rank_{global_rank}'
    if global_rank == 0:
        writer = SummaryWriter(result_dir)
    else:
        writer = None # rankÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± writer Ù†Ø¯Ø§Ø±Ù†Ø¯

    if global_rank == 0:
        print("="*70)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Fine-tuning Ù…Ø¯Ù„ Pruned ResNet50 Ø¨Ø§ DDP Ùˆ Mixed Precision")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ø§ÙÛŒÚ©: {world_size}")
        print(f"   Batch Size Ú©Ù„: {BATCH_SIZE}")
        print("="*70)

    # Ù„ÙˆØ¯ Ù…Ø¯Ù„
    if global_rank == 0:
        print("\nğŸ“¦ Ù„ÙˆØ¯ Ù…Ø¯Ù„ Pruned...")

    input_model_path = '/kaggle/input/m/saraaskari/10k_final/pytorch/default/1/10k_final.pt'
    checkpoint = torch.load(input_model_path, map_location=DEVICE)

    masks_detached = [m.detach().clone() if m is not None else None for m in checkpoint['masks']]

    model = ResNet_50_pruned_hardfakevsreal(masks=masks_detached)
    model.load_state_dict(checkpoint['model_state_dict'])

    model = model.to(DEVICE)
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    if global_rank == 0:
        print(f"âœ… Ù…Ø¯Ù„ Ù„ÙˆØ¯ Ø´Ø¯")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´: {trainable_params:,}")

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    if global_rank == 0:
        print("\nğŸ“Š Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ DataLoaders...")

    train_loader, val_loader, test_loader, train_sampler, val_sampler, test_sampler = create_dataloaders(
        batch_size=BATCH_SIZE_PER_GPU,
        num_workers=2
    )

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² scheduler Ø¨Ø§ warmup
    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
    scheduler = CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=2, eta_min=1e-6
    )

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† GradScaler Ø¨Ø±Ø§ÛŒ Mixed Precision
    scaler = GradScaler(enabled=True)

    # Ø¢Ù…ÙˆØ²Ø´
    if global_rank == 0:
        print("\n" + "="*70)
        print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´")
        print("="*70)

    best_val_acc = 0.0
    best_model_path = f'/kaggle/working/best_pruned_finetuned_ddp_rank_{global_rank}.pt'

    for epoch in range(NUM_EPOCHS):
        train_sampler.set_epoch(epoch)
        val_sampler.set_epoch(epoch)

        if global_rank == 0:
            print(f"\nğŸ“ Epoch {epoch+1}/{NUM_EPOCHS}")
            print(f"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
            print("-" * 70)

        # Ø¢Ù…ÙˆØ²Ø´ - Ø¨Ø¯ÙˆÙ† ACCUMULATION_STEPS
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, DEVICE, scaler, writer, epoch, global_rank
        )
        if global_rank == 0:
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")

        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        val_loss, val_acc = validate(model, val_loader, criterion, DEVICE, writer, epoch, global_rank)
        if global_rank == 0:
            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")

        scheduler.step()

        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ ÙÙ‚Ø· Ø¯Ø± rank 0
        if global_rank == 0:
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.module.state_dict(),
                    'masks': checkpoint['masks'],
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_acc': val_acc,
                    'train_acc': train_acc,
                    'total_params': total_params,
                    'scaler_state_dict': scaler.state_dict()
                }, best_model_path)
                print(f"ğŸ’¾ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (Val Acc: {val_acc:.2f}%)")

    # ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ
    if global_rank == 0:
        print("\n" + "="*70)
        print("ğŸ§ª ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„")
        print("="*70)

        best_checkpoint = torch.load(best_model_path)
        model.module.load_state_dict(best_checkpoint['model_state_dict'])
        scaler.load_state_dict(best_checkpoint.get('scaler_state_dict', {}))

        test_loss, test_acc = validate(model, test_loader, criterion, DEVICE, writer, NUM_EPOCHS, global_rank)
        print(f"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%")

        final_model_path = '/kaggle/working/final_pruned_finetuned_ddp.pt'
        torch.save({
            'model_state_dict': model.module.state_dict(),
            'masks': checkpoint['masks'],
            'test_acc': test_acc,
            'best_val_acc': best_val_acc,
            'total_params': total_params,
            'model_architecture': 'ResNet_50_pruned_hardfakevsreal'
        }, final_model_path)

        print(f"\nâœ… Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± {final_model_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        print(f"ğŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯Ù‚Øª Validation: {best_val_acc:.2f}%")
        print(f"ğŸ“Š Ø¯Ù‚Øª Test: {test_acc:.2f}%")
        print("\n" + "="*70)
        print("ğŸ‰ Fine-tuning Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
        print("="*70)

        writer.close()

    cleanup_ddp()

if __name__ == "__main__":
    main()
