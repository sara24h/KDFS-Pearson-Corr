import torch
import torch.nn as nn
import sys
sys.path.append('/kaggle/working')

from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal

# Ù…Ø³ÛŒØ± Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø¬Ø¯ÛŒØ¯
checkpoint_path = '/kaggle/input/kdfs-140k-pearson-19-shahrivar-data/results/run_resnet50_imagenet_prune1/student_model/resnet50_sparse_best.pt'

print("="*70)
print("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù…Ø¯Ù„ Sparse")
print("="*70)

checkpoint = torch.load(checkpoint_path, map_location='cpu')
sparse_state_dict = checkpoint['student']

def extract_pruned_info_auto(state_dict):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ pruned Ø´Ø¯Ù‡ Ùˆ Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§
    """
    pruned_counts = []
    original_counts = []
    masks = []
    
    # ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ResNet50
    original_filters_per_layer = {
        'layer1': [64, 64, 256] * 3,
        'layer2': [128, 128, 512] * 4,
        'layer3': [256, 256, 1024] * 6,
        'layer4': [512, 512, 2048] * 3,
    }
    
    layer_configs = [
        ('layer1', 3),
        ('layer2', 4),
        ('layer3', 6),
        ('layer4', 3),
    ]
    
    print("\nğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ú©Ù„ ÙˆØ²Ù†â€ŒÙ‡Ø§...")
    
    for layer_name, num_blocks in layer_configs:
        for block_idx in range(num_blocks):
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                
                if key in state_dict:
                    weight = state_dict[key]
                    num_pruned_filters = weight.shape[0]  # ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡
                    
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
                    layer_idx = int(layer_name[-1]) - 1
                    block_conv_idx = block_idx * 3 + (conv_idx - 1)
                    num_original_filters = original_filters_per_layer[layer_name][block_conv_idx]
                    
                    pruned_counts.append(num_pruned_filters)
                    original_counts.append(num_original_filters)
                    
                    # Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©
                    mask = torch.zeros(num_original_filters)
                    mask[:num_pruned_filters] = 1
                    masks.append(mask)
                    
                    print(f"  {key:50s} | {num_pruned_filters:4d}/{num_original_filters:4d} ({100*num_pruned_filters/num_original_filters:.1f}%)")
                else:
                    print(f"âš ï¸  {key} not found!")
    
    return masks, pruned_counts, original_counts

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
masks, pruned_counts, original_counts = extract_pruned_info_auto(sparse_state_dict)

print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡: {len(masks)}")
print(f"âœ… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù†Ø±Ø® Pruning: {100*(1 - sum(pruned_counts)/sum(original_counts)):.2f}%")

# Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
print("\nğŸ“Š Ø¢Ù…Ø§Ø± Pruning:")
print(f"  - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {sum(original_counts):,}")
print(f"  - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {sum(pruned_counts):,}")
print(f"  - Ú©Ø§Ù‡Ø´: {sum(original_counts) - sum(pruned_counts):,} Ù¾Ø§Ø±Ø§Ù…ØªØ±")

# ===========================
# 2. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned")
print("="*70)

try:
    model_pruned = ResNet_50_pruned_hardfakevsreal(masks=masks)
    print("âœ… Ù…Ø¯Ù„ pruned Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯!")
    
    total_params = sum(p.numel() for p in model_pruned.parameters())
    trainable_params = sum(p.numel() for p in model_pruned.parameters() if p.requires_grad)
    
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    print(f"âœ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´: {trainable_params:,}")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ù…Ø¯Ù„: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ===========================
# 3. Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)
# ===========================

print("\n" + "="*70)
print("Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned")
print("="*70)

def load_pruned_weights_improved(model_pruned, sparse_state_dict, masks):
    """
    Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ sparse Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ± input/output channels
    """
    pruned_state_dict = {}
    
    # conv1 Ùˆ bn1
    for key in ['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 
                'bn1.running_var', 'bn1.num_batches_tracked']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    # fc
    for key in ['fc.weight', 'fc.bias']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    mask_idx = 0
    layer_configs = [
        ('layer1', 3),
        ('layer2', 4),
        ('layer3', 6),
        ('layer4', 3),
    ]
    
    for layer_name, num_blocks in layer_configs:
        for block_idx in range(num_blocks):
            # Ø¨Ø±Ø§ÛŒ Ù‡Ø± blockØŒ conv1->conv2->conv3
            for conv_idx in range(1, 4):
                sparse_conv_key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                
                if sparse_conv_key not in sparse_state_dict:
                    continue
                
                sparse_weight = sparse_state_dict[sparse_conv_key]
                current_mask = masks[mask_idx]
                
                # ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø¯Ø± output
                active_out = (current_mask == 1).nonzero(as_tuple=True)[0]
                pruned_weight = sparse_weight[active_out]
                
                # Ù…Ø¯ÛŒØ±ÛŒØª input channels
                # conv1: ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv3 Ø¨Ù„Ø§Ú© Ù‚Ø¨Ù„ÛŒ ÛŒØ§ downsample
                # conv2: ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv1 Ù‡Ù…ÛŒÙ† Ø¨Ù„Ø§Ú©
                # conv3: ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv2 Ù‡Ù…ÛŒÙ† Ø¨Ù„Ø§Ú©
                
                if conv_idx == 1:
                    # conv1 ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv3 Ø¨Ù„Ø§Ú© Ù‚Ø¨Ù„ÛŒ Ø¯Ø§Ø±Ù‡
                    if mask_idx > 0:
                        prev_mask = masks[mask_idx - 1]
                        active_in = (prev_mask == 1).nonzero(as_tuple=True)[0]
                        pruned_weight = pruned_weight[:, active_in]
                
                elif conv_idx == 2:
                    # conv2 ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv1 Ù‡Ù…ÛŒÙ† Ø¨Ù„Ø§Ú©
                    conv1_mask = masks[mask_idx - 1]
                    active_in = (conv1_mask == 1).nonzero(as_tuple=True)[0]
                    pruned_weight = pruned_weight[:, active_in]
                
                elif conv_idx == 3:
                    # conv3 ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² conv2 Ù‡Ù…ÛŒÙ† Ø¨Ù„Ø§Ú©
                    conv2_mask = masks[mask_idx - 1]
                    active_in = (conv2_mask == 1).nonzero(as_tuple=True)[0]
                    pruned_weight = pruned_weight[:, active_in]
                
                pruned_state_dict[sparse_conv_key] = pruned_weight
                
                # BatchNorm
                bn_keys = [
                    f'{layer_name}.{block_idx}.bn{conv_idx}.weight',
                    f'{layer_name}.{block_idx}.bn{conv_idx}.bias',
                    f'{layer_name}.{block_idx}.bn{conv_idx}.running_mean',
                    f'{layer_name}.{block_idx}.bn{conv_idx}.running_var',
                    f'{layer_name}.{block_idx}.bn{conv_idx}.num_batches_tracked',
                ]
                
                for bn_key in bn_keys:
                    if bn_key in sparse_state_dict:
                        if 'num_batches_tracked' in bn_key:
                            pruned_state_dict[bn_key] = sparse_state_dict[bn_key]
                        else:
                            pruned_state_dict[bn_key] = sparse_state_dict[bn_key][active_out]
                
                mask_idx += 1
            
            # downsample
            downsample_keys = [
                f'{layer_name}.{block_idx}.downsample.0.weight',
                f'{layer_name}.{block_idx}.downsample.1.weight',
                f'{layer_name}.{block_idx}.downsample.1.bias',
                f'{layer_name}.{block_idx}.downsample.1.running_mean',
                f'{layer_name}.{block_idx}.downsample.1.running_var',
                f'{layer_name}.{block_idx}.downsample.1.num_batches_tracked',
            ]
            
            for key in downsample_keys:
                if key in sparse_state_dict:
                    pruned_state_dict[key] = sparse_state_dict[key]
    
    return pruned_state_dict

try:
    print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ ÙˆØ²Ù†â€ŒÙ‡Ø§...")
    pruned_weights = load_pruned_weights_improved(model_pruned, sparse_state_dict, masks)
    print(f"âœ… ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: {len(pruned_weights)} Ú©Ù„ÛŒØ¯")
    
    # Ù„ÙˆØ¯
    missing, unexpected = model_pruned.load_state_dict(pruned_weights, strict=False)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯")
    
    if missing:
        print(f"âš ï¸  Missing keys ({len(missing)}): {missing[:5]}...")
    if unexpected:
        print(f"âš ï¸  Unexpected keys ({len(unexpected)}): {unexpected[:5]}...")
    
    # ØªØ³Øª
    print("\nğŸ§ª ØªØ³Øª Ù…Ø¯Ù„...")
    model_pruned.eval()
    with torch.no_grad():
        dummy_input = torch.randn(2, 3, 224, 224)
        output, features = model_pruned(dummy_input)
        print(f"âœ… ØªØ³Øª Ù…ÙˆÙÙ‚!")
        print(f"   - Output: {output.shape}")
        print(f"   - Features: {len(features)} Ù…Ù¾")
        for i, feat in enumerate(features):
            print(f"      Feature {i}: {feat.shape}")
    
    # ===========================
    # 4. Ø°Ø®ÛŒØ±Ù‡
    # ===========================
    
    print("\n" + "="*70)
    print("Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„")
    print("="*70)
    
    save_dir = '/kaggle/working'
    
    # Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ú©Ø§Ù…Ù„
    checkpoint_full = {
        'model_state_dict': model_pruned.state_dict(),
        'masks': masks,
        'pruned_counts': pruned_counts,
        'original_counts': original_counts,
        'total_params': total_params,
        'model_architecture': 'ResNet_50_pruned_hardfakevsreal',
        'pruning_rate': 1 - sum(pruned_counts)/sum(original_counts),
    }
    
    save_path_full = f'{save_dir}/resnet50_pruned_complete.pt'
    torch.save(checkpoint_full, save_path_full)
    
    # ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§
    save_path_weights = f'{save_dir}/resnet50_pruned_weights.pt'
    torch.save(model_pruned.state_dict(), save_path_weights)
    
    import os
    size_full = os.path.getsize(save_path_full) / (1024**2)
    size_weights = os.path.getsize(save_path_weights) / (1024**2)
    
    print(f"âœ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print(f"   ğŸ“¦ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ú©Ø§Ù…Ù„: {save_path_full} ({size_full:.2f} MB)")
    print(f"   ğŸ“¦ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§: {save_path_weights} ({size_weights:.2f} MB)")
    
    print("\nğŸ’¡ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:")
    print("```python")
    print("checkpoint = torch.load('resnet50_pruned_complete.pt')")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=checkpoint['masks'])")
    print("model.load_state_dict(checkpoint['model_state_dict'])")
    print("```")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
print("âœ… Ù¾Ø§ÛŒØ§Ù†")
print("="*70)
