import torch
import torch.nn as nn
import sys
sys.path.append('/kaggle/working')

from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal

checkpoint_path = '/kaggle/input/kdfs-10k-pearson-19-shahrivar-314-epochs/results/run_resnet50_imagenet_prune1/student_model/finetune_ResNet_50_sparse_best.pt'

print("="*70)
print("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù…Ø¯Ù„ Sparse")
print("="*70)

checkpoint = torch.load(checkpoint_path, map_location='cpu')
sparse_state_dict = checkpoint['student']

def extract_masks_automatically(state_dict):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ shape ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ conv Ø¯Ø± checkpoint
    """
    masks = []
    pruned_filters = []
    original_filters = []
    
    # ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø®ØªØ§Ø± ResNet50 - ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    resnet50_structure = {
        'layer1': {'blocks': 3, 'filters': [64, 64, 256]},
        'layer2': {'blocks': 4, 'filters': [128, 128, 512]},
        'layer3': {'blocks': 6, 'filters': [256, 256, 1024]},
        'layer4': {'blocks': 3, 'filters': [512, 512, 2048]}
    }
    
    print("\nğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ checkpoint...")
    
    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:
        num_blocks = resnet50_structure[layer_name]['blocks']
        standard_filters = resnet50_structure[layer_name]['filters']
        
        for block_idx in range(num_blocks):
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                conv_key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                
                if conv_key in state_dict:
                    # Ú¯Ø±ÙØªÙ† shape ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ²Ù†
                    weight_shape = state_dict[conv_key].shape
                    pruned_count = weight_shape[0]  # ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (out_channels)
                    original_count = standard_filters[conv_idx - 1]
                    
                    # Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©
                    mask = torch.zeros(original_count)
                    mask[:pruned_count] = 1
                    
                    masks.append(mask)
                    pruned_filters.append(pruned_count)
                    original_filters.append(original_count)
                    
                    print(f"  âœ“ {conv_key}: {pruned_count}/{original_count} ÙÛŒÙ„ØªØ± Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡")
                else:
                    print(f"  âš  {conv_key} Ø¯Ø± checkpoint ÛŒØ§ÙØª Ù†Ø´Ø¯!")
    
    return masks, pruned_filters, original_filters

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø±
masks, pruned_counts, original_counts = extract_masks_automatically(sparse_state_dict)

print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡: {len(masks)}")
print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:")
print(f"   - Ù…Ø¬Ù…ÙˆØ¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {sum(original_counts):,}")
print(f"   - Ù…Ø¬Ù…ÙˆØ¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {sum(pruned_counts):,}")
print(f"   - Ù†Ø±Ø® Ø­Ø°Ù: {(1 - sum(pruned_counts)/sum(original_counts))*100:.2f}%")

# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
print("\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§:")
for i in range(min(10, len(masks))):
    remaining = int(masks[i].sum())
    total = len(masks[i])
    print(f"  Mask {i:2d}: {remaining:4d}/{total:4d} ({remaining/total*100:5.1f}%)")

# ===========================
# 2. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned")
print("="*70)

try:
    model_pruned = ResNet_50_pruned_hardfakevsreal(masks=masks)
    print("âœ… Ù…Ø¯Ù„ pruned Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯!")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    total_params = sum(p.numel() for p in model_pruned.parameters())
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ pruned: {total_params:,}")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ù…Ø¯Ù„: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ===========================
# 3. Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned")
print("="*70)

def load_pruned_weights(model_pruned, sparse_state_dict, masks):
    """
    Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ sparse Ø¯Ø± Ù…Ø¯Ù„ pruned
    """
    pruned_state_dict = {}
    
    # conv1 Ùˆ bn1 (Ù‚Ø¨Ù„ Ø§Ø² layer1)
    for key in ['conv1.weight', 'bn1.weight', 'bn1.bias', 
                'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    # fc (Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±)
    for key in ['fc.weight', 'fc.bias']:
        if key in sparse_state_dict:
            pruned_state_dict[key] = sparse_state_dict[key]
    
    mask_idx = 0
    layer_configs = [
        ('layer1', 3), ('layer2', 4), ('layer3', 6), ('layer4', 3)
    ]
    
    for layer_name, num_blocks in layer_configs:
        for block_idx in range(num_blocks):
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                sparse_conv_key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                
                if sparse_conv_key in sparse_state_dict:
                    sparse_weight = sparse_state_dict[sparse_conv_key]
                    mask = masks[mask_idx]
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡
                    active_filters = (mask == 1).nonzero(as_tuple=True)[0]
                    pruned_weight = sparse_weight[active_filters]
                    
                    # Ø§Ú¯Ø± conv2 ÛŒØ§ conv3ØŒ input channels Ù‡Ù… Ø¨Ø§ÛŒØ¯ prune Ø¨Ø´Ù‡
                    if conv_idx > 1 and mask_idx > 0:
                        prev_mask = masks[mask_idx - 1]
                        active_in_channels = (prev_mask == 1).nonzero(as_tuple=True)[0]
                        pruned_weight = pruned_weight[:, active_in_channels]
                    
                    pruned_state_dict[sparse_conv_key] = pruned_weight
                    
                    # BatchNorm
                    bn_prefix = f'{layer_name}.{block_idx}.bn{conv_idx}'
                    for bn_key in ['weight', 'bias', 'running_mean', 'running_var', 'num_batches_tracked']:
                        full_key = f'{bn_prefix}.{bn_key}'
                        if full_key in sparse_state_dict:
                            if bn_key == 'num_batches_tracked':
                                pruned_state_dict[full_key] = sparse_state_dict[full_key]
                            else:
                                pruned_state_dict[full_key] = sparse_state_dict[full_key][active_filters]
                    
                    mask_idx += 1
            
            # downsample (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡)
            downsample_prefix = f'{layer_name}.{block_idx}.downsample'
            if f'{downsample_prefix}.0.weight' in sparse_state_dict:
                for key in sparse_state_dict.keys():
                    if key.startswith(downsample_prefix):
                        pruned_state_dict[key] = sparse_state_dict[key]
    
    return pruned_state_dict

try:
    pruned_weights = load_pruned_weights(model_pruned, sparse_state_dict, masks)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ pruned Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯: {len(pruned_weights)} Ú©Ù„ÛŒØ¯")
    
    # Ù„ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„
    missing, unexpected = model_pruned.load_state_dict(pruned_weights, strict=False)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯")
    print(f"   - Missing keys: {len(missing)}")
    if len(missing) > 0:
        print(f"   - Ø§ÙˆÙ„ÛŒÙ† missing keys: {missing[:5]}")
    print(f"   - Unexpected keys: {len(unexpected)}")
    
    # ØªØ³Øª
    model_pruned.eval()
    with torch.no_grad():
        dummy_input = torch.randn(2, 3, 224, 224)
        output, features = model_pruned(dummy_input)
        print(f"\nâœ… ØªØ³Øª Ù…ÙˆÙÙ‚!")
        print(f"   - Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ: {output.shape}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ feature maps: {len(features)}")
    
    # ===========================
    # 4. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned
    # ===========================
    
    print("\n" + "="*70)
    print("Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned")
    print("="*70)
    
    save_path = '/kaggle/working/resnet50_pruned_model.pt'
    
    checkpoint_to_save = {
        'model_state_dict': model_pruned.state_dict(),
        'masks': masks,
        'pruned_counts': pruned_counts,
        'original_counts': original_counts,
        'total_params': total_params,
        'model_architecture': 'ResNet_50_pruned_hardfakevsreal',
        'compression_ratio': sum(pruned_counts) / sum(original_counts)
    }
    
    torch.save(checkpoint_to_save, save_path)
    print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {save_path}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… ÙØ§ÛŒÙ„
    import os
    file_size_mb = os.path.getsize(save_path) / (1024 * 1024)
    print(f"âœ… Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {file_size_mb:.2f} MB")
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§
    save_path_weights = '/kaggle/working/resnet50_pruned_weights_only.pt'
    torch.save(model_pruned.state_dict(), save_path_weights)
    file_size_weights_mb = os.path.getsize(save_path_weights) / (1024 * 1024)
    print(f"âœ… ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {save_path_weights}")
    print(f"âœ… Ø­Ø¬Ù… ÙØ§ÛŒÙ„ (ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§): {file_size_weights_mb:.2f} MB")
    
    print("\nğŸ“¦ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§: {len(masks)}")
    print(f"   - Ù†Ø³Ø¨Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ: {checkpoint_to_save['compression_ratio']:.2%}")
    print(f"   - Ù…Ø¹Ù…Ø§Ø±ÛŒ: ResNet_50_pruned_hardfakevsreal")
    
    print("\nğŸ’¡ Ù†Ø­ÙˆÙ‡ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù†:")
    print("```python")
    print("checkpoint = torch.load('resnet50_pruned_model.pt')")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=checkpoint['masks'])")
    print("model.load_state_dict(checkpoint['model_state_dict'])")
    print("model.eval()")
    print("```")
        
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
print("âœ… ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
print("="*70)
