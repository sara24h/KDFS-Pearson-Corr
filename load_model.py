import torch
import os

# ===========================
# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned")
print("="*70)

# Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡
save_dir = '/kaggle/working/saved_models'
os.makedirs(save_dir, exist_ok=True)

# 1. Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ù…Ù„ (Ù…Ø¯Ù„ + Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ + Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ)
full_save_path = os.path.join(save_dir, 'resnet50_pruned_full.pth')

torch.save({
    'model_state_dict': model_pruned.state_dict(),
    'masks': masks,
    'pruned_filters': pruned_counts,
    'original_filters': original_counts,
    'architecture': 'ResNet50_Bottleneck_pruned',
    'num_classes': 1,
    'num_params': sum(p.numel() for p in model_pruned.parameters()),
    'best_prec1': checkpoint.get('best_prec1_after_finetune', None),
    'info': {
        'pruning_method': 'filter_pruning',
        'total_params': sum(p.numel() for p in model_pruned.parameters()),
        'compression_ratio': 23.51e6 / sum(p.numel() for p in model_pruned.parameters()),
        'params_reduction_percent': (1 - sum(p.numel() for p in model_pruned.parameters()) / 23.51e6) * 100
    }
}, full_save_path)

print(f"âœ… Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {full_save_path}")
print(f"   Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {os.path.getsize(full_save_path) / (1024**2):.2f} MB")

# 2. Ø°Ø®ÛŒØ±Ù‡ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ inference Ø³Ø¨Ú©â€ŒØªØ±)
weights_only_path = os.path.join(save_dir, 'resnet50_pruned_weights.pth')

torch.save(model_pruned.state_dict(), weights_only_path)

print(f"âœ… ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {weights_only_path}")
print(f"   Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {os.path.getsize(weights_only_path) / (1024**2):.2f} MB")

# 3. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
masks_path = os.path.join(save_dir, 'resnet50_masks.pth')

torch.save({
    'masks': masks,
    'pruned_filters': pruned_counts,
    'original_filters': original_counts
}, masks_path)

print(f"âœ… Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯: {masks_path}")
print(f"   Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {os.path.getsize(masks_path) / (1024):.2f} KB")

# ===========================
# ØªÙˆØ§Ø¨Ø¹ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù†
# ===========================

print("\n" + "="*70)
print("ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡")
print("="*70)

def load_full_pruned_model(checkpoint_path):
    """
    Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„ pruned (Ø¨Ø§ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§)
    
    Args:
        checkpoint_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
    
    Returns:
        model: Ù…Ø¯Ù„ pruned Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        info: Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
    """
    print(f"Ø¯Ø± Ø­Ø§Ù„ Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ø²: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Ø³Ø§Ø®Øª Ù…Ø¯Ù„
    from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal
    masks = checkpoint['masks']
    model = ResNet_50_pruned_hardfakevsreal(masks=masks)
    
    # Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù„ÙˆØ¯ Ø´Ø¯!")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {checkpoint['num_params']:,}")
    if 'best_prec1' in checkpoint and checkpoint['best_prec1'] is not None:
        print(f"   - Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯Ù‚Øª: {checkpoint['best_prec1']:.2f}%")
    
    return model, checkpoint.get('info', {})

def load_weights_only(weights_path, masks_path):
    """
    Ù„ÙˆØ¯ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ (Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¯Ø§Ø±Ù‡)
    
    Args:
        weights_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ²Ù†â€ŒÙ‡Ø§
        masks_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§
    
    Returns:
        model: Ù…Ø¯Ù„ pruned Ù„ÙˆØ¯ Ø´Ø¯Ù‡
    """
    print(f"Ø¯Ø± Ø­Ø§Ù„ Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø§Ø²: {weights_path}")
    print(f"Ø¯Ø± Ø­Ø§Ù„ Ù„ÙˆØ¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø§Ø²: {masks_path}")
    
    # Ù„ÙˆØ¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§
    masks_data = torch.load(masks_path, map_location='cpu')
    masks = masks_data['masks']
    
    # Ø³Ø§Ø®Øª Ù…Ø¯Ù„
    from model.pruned_model.ResNet_pruned import ResNet_50_pruned_hardfakevsreal
    model = ResNet_50_pruned_hardfakevsreal(masks=masks)
    
    # Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§
    state_dict = torch.load(weights_path, map_location='cpu')
    model.load_state_dict(state_dict)
    model.eval()
    
    print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù„ÙˆØ¯ Ø´Ø¯!")
    
    return model

# Ù†Ù…Ø§ÛŒØ´ Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
print("\nÙ†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:")
print("-" * 70)
print("""
# Ø±ÙˆØ´ 1: Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„
model, info = load_full_pruned_model('/kaggle/working/saved_models/resnet50_pruned_full.pth')
print(f"Ù†Ø³Ø¨Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ: {info['compression_ratio']:.2f}x")

# Ø±ÙˆØ´ 2: Ù„ÙˆØ¯ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§
model = load_weights_only(
    '/kaggle/working/saved_models/resnet50_pruned_weights.pth',
    '/kaggle/working/saved_models/resnet50_masks.pth'
)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
from PIL import Image
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

image = Image.open('test.jpg').convert('RGB')
image_tensor = transform(image).unsqueeze(0)

with torch.no_grad():
    output, _ = model(image_tensor)
    prob = torch.sigmoid(output).item()
    prediction = "Fake" if prob > 0.5 else "Real"
    print(f"Prediction: {prediction} (probability: {prob:.4f})")
""")

# ===========================
# ØªØ³Øª Ù„ÙˆØ¯ Ù…Ø¬Ø¯Ø¯
# ===========================

print("\n" + "="*70)
print("ØªØ³Øª Ù„ÙˆØ¯ Ù…Ø¬Ø¯Ø¯ Ù…Ø¯Ù„")
print("="*70)

try:
    # ØªØ³Øª Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„
    loaded_model, info = load_full_pruned_model(full_save_path)
    
    # ØªØ³Øª Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    with torch.no_grad():
        test_input = torch.randn(1, 3, 224, 224)
        
        # Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ
        output_original, _ = model_pruned(test_input)
        
        # Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        output_loaded, _ = loaded_model(test_input)
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡
        diff = (output_original - output_loaded).abs().item()
        
        print(f"\nâœ… ØªØ³Øª Ù„ÙˆØ¯ Ù…ÙˆÙÙ‚!")
        print(f"   - Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ: {output_original.item():.6f}")
        print(f"   - Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ù„ÙˆØ¯ Ø´Ø¯Ù‡: {output_loaded.item():.6f}")
        print(f"   - ØªÙØ§ÙˆØª: {diff:.8f}")
        
        if diff < 1e-6:
            print(f"   âœ… Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† Ù‡Ø³ØªÙ†!")
        else:
            print(f"   âš ï¸ ØªÙØ§ÙˆØª Ø¬Ø²Ø¦ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡ (Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„)")
            
    print(f"\nğŸ“Š Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„:")
    print(f"   - Ù†Ø³Ø¨Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ: {info['compression_ratio']:.2f}x")
    print(f"   - Ú©Ø§Ù‡Ø´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {info['params_reduction_percent']:.2f}%")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
print("âœ… Ù‡Ù…Ù‡ Ú†ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
print("="*70)
print(f"\nÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
print(f"  1. {full_save_path}")
print(f"  2. {weights_only_path}")
print(f"  3. {masks_path}")
print("\nğŸ¯ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§ÛŒÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒ Ùˆ Ø¯Ø± Ù‡Ø± Ø¬Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ!")
print("="*70)
