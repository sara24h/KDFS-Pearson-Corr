import torch
import torch.nn as nn
import sys
sys.path.append('/kaggle/working')

from model.pruned_model.ResNet_pruned  import ResNet_50_pruned_hardfakevsreal

checkpoint_path = '/kaggle/input/kdfs-10k-pearson-19-shahrivar-314-epochs/results/run_resnet50_imagenet_prune1/student_model/finetune_ResNet_50_sparse_best.pt'

print("="*70)
print("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù…Ø¯Ù„ Sparse")
print("="*70)

checkpoint = torch.load(checkpoint_path, map_location='cpu')
sparse_state_dict = checkpoint['student']

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø§Ø² ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ conv
def extract_masks_from_sparse_model(state_dict):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡
    Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ load_model.py Ù…ÛŒØ¯ÙˆÙ†ÛŒÙ… Ú©Ù‡ Ù…Ø«Ù„Ø§Ù‹ conv1 Ø§Ø² 64 Ø¨Ù‡ 20 ÙÛŒÙ„ØªØ± Ø±Ø³ÛŒØ¯Ù‡
    """
    masks = []
    
    # ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø®ØªØ§Ø± ResNet50 Bottleneck
    # layer1: 3 blocks Ã— 3 convs = 9 masks
    # layer2: 4 blocks Ã— 3 convs = 12 masks
    # layer3: 6 blocks Ã— 3 convs = 18 masks
    # layer4: 3 blocks Ã— 3 convs = 9 masks
    # Ø¬Ù…Ø¹: 48 masks
    
    # Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ load_model.py Ù…ÛŒØ¯ÙˆÙ†ÛŒÙ… ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ pruned Ø´Ø¯Ù‡:
    # Ø§ÛŒÙ† Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ùˆ Ø§Ø² "Pruned weight shape" Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯ÛŒÙ…
    pruned_filters = [
        # layer1.0
        20, 23, 94,
        # layer1.1
        13, 27, 91,
        # layer1.2
        27, 24, 82,
        # layer2.0
        44, 42, 92,
        # layer2.1
        47, 29, 94,
        # layer2.2
        37, 28, 71,
        # layer2.3
        43, 34, 56,
        # layer3.0
        65, 42, 66,
        # layer3.1
        63, 31, 66,
        # layer3.2
        59, 17, 60,
        # layer3.3
        40, 19, 40,
        # layer3.4
        30, 10, 31,
        # layer3.5
        29, 19, 29,
        # layer4.0
        69, 17, 62,
        # layer4.1
        59, 18, 83,
        # layer4.2
        72, 47, 89
    ]
    
    # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¯Ø± ResNet50 Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    original_filters = [
        # layer1: 3 blocks
        64, 64, 256,  # block 0
        64, 64, 256,  # block 1
        64, 64, 256,  # block 2
        # layer2: 4 blocks
        128, 128, 512,  # block 0
        128, 128, 512,  # block 1
        128, 128, 512,  # block 2
        128, 128, 512,  # block 3
        # layer3: 6 blocks
        256, 256, 1024,  # block 0
        256, 256, 1024,  # block 1
        256, 256, 1024,  # block 2
        256, 256, 1024,  # block 3
        256, 256, 1024,  # block 4
        256, 256, 1024,  # block 5
        # layer4: 3 blocks
        512, 512, 2048,  # block 0
        512, 512, 2048,  # block 1
        512, 512, 2048,  # block 2
    ]
    
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²: {len(original_filters)}")
    print(f"ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ pruned Ø´Ø¯Ù‡: {len(pruned_filters)}")
    
    # Ø³Ø§Ø®Øª Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§
    for orig_filters, pruned_count in zip(original_filters, pruned_filters):
        mask = torch.zeros(orig_filters)
        # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ† ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø­ÙØ¸ Ø´Ø¯Ù†
        mask[:pruned_count] = 1
        masks.append(mask)
        
    return masks, pruned_filters, original_filters

masks, pruned_counts, original_counts = extract_masks_from_sparse_model(sparse_state_dict)

print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡: {len(masks)}")

# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
print("\nÙ†Ù…ÙˆÙ†Ù‡ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§:")
for i in range(min(5, len(masks))):
    print(f"  Mask {i}: {masks[i].shape}, ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {int(masks[i].sum())}/{len(masks[i])}")

# ===========================
# 2. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Pruned")
print("="*70)

try:
    model_pruned = ResNet_50_pruned_hardfakevsreal(masks=masks)
    print("âœ… Ù…Ø¯Ù„ pruned Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯!")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    total_params = sum(p.numel() for p in model_pruned.parameters())
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ pruned: {total_params:,}")
    
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ù…Ø¯Ù„: {e}")
    print("\nÙ…Ù…Ú©Ù†Ù‡ Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ù‡ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø³ØªÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒÙ…...")

# ===========================
# 3. Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned
# ===========================

print("\n" + "="*70)
print("Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ Pruned")
print("="*70)

def load_pruned_weights(model_pruned, sparse_state_dict, masks):
    """
    Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ sparse Ø¯Ø± Ù…Ø¯Ù„ pruned
    Ø¨Ø§ÛŒØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø±Ùˆ Ø§Ø² Ø´Ú©Ù„ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ø´Ú©Ù„ pruned ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…
    """
    pruned_state_dict = {}
    
    # conv1 Ùˆ bn1 (Ù‚Ø¨Ù„ Ø§Ø² layer1)
    if 'conv1.weight' in sparse_state_dict:
        pruned_state_dict['conv1.weight'] = sparse_state_dict['conv1.weight']
    if 'bn1.weight' in sparse_state_dict:
        pruned_state_dict['bn1.weight'] = sparse_state_dict['bn1.weight']
        pruned_state_dict['bn1.bias'] = sparse_state_dict['bn1.bias']
        pruned_state_dict['bn1.running_mean'] = sparse_state_dict['bn1.running_mean']
        pruned_state_dict['bn1.running_var'] = sparse_state_dict['bn1.running_var']
        pruned_state_dict['bn1.num_batches_tracked'] = sparse_state_dict['bn1.num_batches_tracked']
    
    # fc (Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±)
    if 'fc.weight' in sparse_state_dict:
        pruned_state_dict['fc.weight'] = sparse_state_dict['fc.weight']
        pruned_state_dict['fc.bias'] = sparse_state_dict['fc.bias']
    
    mask_idx = 0
    layer_configs = [
        ('layer1', 3),  # 3 blocks
        ('layer2', 4),  # 4 blocks
        ('layer3', 6),  # 6 blocks
        ('layer4', 3),  # 3 blocks
    ]
    
    for layer_name, num_blocks in layer_configs:
        for block_idx in range(num_blocks):
            for conv_idx in range(1, 4):  # conv1, conv2, conv3
                # Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ sparse
                sparse_conv_key = f'{layer_name}.{block_idx}.conv{conv_idx}.weight'
                sparse_bn_key = f'{layer_name}.{block_idx}.bn{conv_idx}.weight'
                
                if sparse_conv_key in sparse_state_dict:
                    sparse_weight = sparse_state_dict[sparse_conv_key]
                    mask = masks[mask_idx]
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡
                    active_filters = (mask == 1).nonzero(as_tuple=True)[0]
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„
                    pruned_weight = sparse_weight[active_filters]
                    
                    # Ø§Ú¯Ø± conv1 ÛŒØ§ conv2 Ø¨ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ input channels Ù‡Ù… prune Ø¨Ø´Ù‡
                    if conv_idx > 1 and mask_idx > 0:
                        prev_mask = masks[mask_idx - 1]
                        active_in_channels = (prev_mask == 1).nonzero(as_tuple=True)[0]
                        pruned_weight = pruned_weight[:, active_in_channels]
                    
                    pruned_state_dict[sparse_conv_key] = pruned_weight
                    
                    # BatchNorm
                    if sparse_bn_key in sparse_state_dict:
                        pruned_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.weight'] = \
                            sparse_state_dict[sparse_bn_key][active_filters]
                        pruned_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.bias'] = \
                            sparse_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.bias'][active_filters]
                        pruned_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.running_mean'] = \
                            sparse_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.running_mean'][active_filters]
                        pruned_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.running_var'] = \
                            sparse_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.running_var'][active_filters]
                        pruned_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.num_batches_tracked'] = \
                            sparse_state_dict[f'{layer_name}.{block_idx}.bn{conv_idx}.num_batches_tracked']
                    
                    mask_idx += 1
            
            # downsample (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡)
            downsample_conv_key = f'{layer_name}.{block_idx}.downsample.0.weight'
            if downsample_conv_key in sparse_state_dict:
                pruned_state_dict[downsample_conv_key] = sparse_state_dict[downsample_conv_key]
                pruned_state_dict[f'{layer_name}.{block_idx}.downsample.1.weight'] = \
                    sparse_state_dict[f'{layer_name}.{block_idx}.downsample.1.weight']
                pruned_state_dict[f'{layer_name}.{block_idx}.downsample.1.bias'] = \
                    sparse_state_dict[f'{layer_name}.{block_idx}.downsample.1.bias']
                pruned_state_dict[f'{layer_name}.{block_idx}.downsample.1.running_mean'] = \
                    sparse_state_dict[f'{layer_name}.{block_idx}.downsample.1.running_mean']
                pruned_state_dict[f'{layer_name}.{block_idx}.downsample.1.running_var'] = \
                    sparse_state_dict[f'{layer_name}.{block_idx}.downsample.1.running_var']
                pruned_state_dict[f'{layer_name}.{block_idx}.downsample.1.num_batches_tracked'] = \
                    sparse_state_dict[f'{layer_name}.{block_idx}.downsample.1.num_batches_tracked']
    
    return pruned_state_dict

try:
    pruned_weights = load_pruned_weights(model_pruned, sparse_state_dict, masks)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ pruned Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯: {len(pruned_weights)} Ú©Ù„ÛŒØ¯")
    
    # Ù„ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„
    missing, unexpected = model_pruned.load_state_dict(pruned_weights, strict=False)
    print(f"âœ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯")
    print(f"   - Missing keys: {len(missing)}")
    print(f"   - Unexpected keys: {len(unexpected)}")
    
    # ØªØ³Øª
    model_pruned.eval()
    with torch.no_grad():
        dummy_input = torch.randn(2, 3, 224, 224)
        output, features = model_pruned(dummy_input)
        print(f"\nâœ… ØªØ³Øª Ù…ÙˆÙÙ‚!")
        print(f"   - Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ: {output.shape}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ feature maps: {len(features)}")
    
    # ===========================
    # 4. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned
    # ===========================
    
    print("\n" + "="*70)
    print("Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Pruned")
    print("="*70)
    
    # Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
    save_path = '/kaggle/working/resnet50_pruned_model.pt'
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„ (Ø´Ø§Ù…Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ + ÙˆØ²Ù†â€ŒÙ‡Ø§)
    checkpoint_to_save = {
        'model_state_dict': model_pruned.state_dict(),
        'masks': masks,
        'pruned_counts': pruned_counts,
        'original_counts': original_counts,
        'total_params': total_params,
        'model_architecture': 'ResNet_50_pruned_hardfakevsreal'
    }
    
    torch.save(checkpoint_to_save, save_path)
    print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {save_path}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… ÙØ§ÛŒÙ„
    import os
    file_size_mb = os.path.getsize(save_path) / (1024 * 1024)
    print(f"âœ… Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {file_size_mb:.2f} MB")
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ (ÙØ§ÛŒÙ„ Ø³Ø¨Ú©â€ŒØªØ±)
    save_path_weights = '/kaggle/working/resnet50_pruned_weights_only.pt'
    torch.save(model_pruned.state_dict(), save_path_weights)
    file_size_weights_mb = os.path.getsize(save_path_weights) / (1024 * 1024)
    print(f"âœ… ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {save_path_weights}")
    print(f"âœ… Ø­Ø¬Ù… ÙØ§ÛŒÙ„ (ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§): {file_size_weights_mb:.2f} MB")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
    print("\nğŸ“¦ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§: {len(masks)}")
    print(f"   - Ù…Ø¹Ù…Ø§Ø±ÛŒ: ResNet_50_pruned_hardfakevsreal")
    
    print("\nğŸ’¡ Ù†Ø­ÙˆÙ‡ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù†:")
    print("# Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ (Ø¨Ø§ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§)")
    print(f"checkpoint = torch.load('{save_path}')")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=checkpoint['masks'])")
    print("model.load_state_dict(checkpoint['model_state_dict'])")
    print("\n# ÛŒØ§ ÙÙ‚Ø· Ù„ÙˆØ¯ ÙˆØ²Ù†â€ŒÙ‡Ø§ (Ø§Ú¯Ø± Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø§Ø±ÛŒØ¯)")
    print("model = ResNet_50_pruned_hardfakevsreal(masks=masks)")
    print(f"model.load_state_dict(torch.load('{save_path_weights}'))")
        
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
